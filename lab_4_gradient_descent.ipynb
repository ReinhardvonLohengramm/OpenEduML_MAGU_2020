{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "snxSUWXD7q_p"
   },
   "source": [
    "## Домашнее задание по неделе 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j-8OLsAV7wrQ"
   },
   "source": [
    "Как было рассказано на лекции, стохастический градиентый спуск сходится быстрее, чем полный, хотя и менее стабильно. В этом задании вам предлагается реализовать стохастический градиентный спуск и сравнить его с точным вычислением весов линейной модели по скорости работы и значению метрики качества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgQyWw5o7Nej"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bGD1wQgMruJw"
   },
   "source": [
    "### Задание 0\n",
    "\n",
    "Реализуйте класс ```LinearRegressionSGD``` c обучением и и применением линейной регрессии, построенной с помощью стохастического градиентного спуска, с заданным интерфейсом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZxdV27R9-uc"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearRegressionSGD(BaseEstimator):\n",
    "    def __init__(self, epsilon = 1e-4, max_steps = 100, w0=None, alpha = 1e-4):\n",
    "        \"\"\"\n",
    "        epsilon: разница для нормы изменения весов \n",
    "        max_steps: максимальное количество шагов в градиентном спуске\n",
    "        w0: np.array (d,) - начальные веса\n",
    "        alpha: шаг обучения\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.max_steps = max_steps\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.w_history = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        y: np.array (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        l, d = X.shape\n",
    "        curindex = np.random.randint(0, l)\n",
    "        ## На каждом шаге градиентного спуска веса необходимо добавлять в w_history\n",
    "        if self.w0 is None:\n",
    "          self.w0 = np.zeros(d)\n",
    "        self.w = self.w0\n",
    "        for step in range(self.max_steps):\n",
    "          self.w_history.append(self.w)\n",
    "          print(X[curindex])\n",
    "          new_w = self.w - self.alpha * self.calc_gradient(X[curindex], y[curindex])\n",
    "\n",
    "          if (np.linalg.norm(new_w - self.w) <=  self.epsilon):\n",
    "            self.w = new_w\n",
    "            break\n",
    "          self.w = new_w\n",
    "          curindex = np.random.randint(0, l)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        ---\n",
    "        output: np.array (l)\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.dot(X, self.w)\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        y: np.array (l)\n",
    "        ---\n",
    "        output: np.array (d)\n",
    "        \"\"\"\n",
    "        grad = 2 * np.dot(X.T, np.dot(X, self.w - y))\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SNOm9-bXpdT3"
   },
   "source": [
    "Проверять работу мы будем на имеющемся в sklearn наборе данных boston: в нём нужно по информации о доме предсказать его стоимость."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c24JCwes9-pe"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), y, test_size = 0.3, random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9eIJwWnInXnr"
   },
   "source": [
    "### Задание 1\n",
    "\n",
    "Метрикой качества в нашей задаче будет MAPE - Mean Absolute Percentage Error. Реализуйте её с заданным интефейсом и вычислите \n",
    "```MAPE(y_test, y_0)```, где ```y_0 = (mean(y_test), mean(y_test), ...)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "znoDavxyuLsi"
   },
   "outputs": [],
   "source": [
    "def MAPE(y_true, y_pred):\n",
    "    \"\"\"\n",
    "        y_true: np.array (l)\n",
    "        y_pred: np.array (l)\n",
    "        ---\n",
    "        output: float [0, +inf)\n",
    "    \"\"\"\n",
    "    return np.abs((y_true - y_pred)/y_true).mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e6mTAykeojwp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result: 37.41588297684096"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = y_test.shape\n",
    "y_0 = np.full(l, y_test.mean())\n",
    "MAPE(y_test, y_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2nNy2ITxuMKf"
   },
   "source": [
    "### Задание 2 \n",
    "\n",
    "Обучите ```LinearRegressionSGD``` с базовыми параметрами на тренировочном наборе данных (```X_train```, ```y_train```), сделайте предсказание на тестовых данных ```X_test```, записав результат в переменную ```y_pred_sgd```  и вычислите ошибку MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7BIHwAwUvB-N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1329e-01 3.0000e+01 4.9300e+00 0.0000e+00 4.2800e-01 6.8970e+00\n",
      " 5.4300e+01 6.3361e+00 6.0000e+00 3.0000e+02 1.6600e+01 3.9125e+02\n",
      " 1.1380e+01]\n",
      "[3.3147e-01 0.0000e+00 6.2000e+00 0.0000e+00 5.0700e-01 8.2470e+00\n",
      " 7.0400e+01 3.6519e+00 8.0000e+00 3.0700e+02 1.7400e+01 3.7895e+02\n",
      " 3.9500e+00]\n",
      "[2.6363e-01 0.0000e+00 8.5600e+00 0.0000e+00 5.2000e-01 6.2290e+00\n",
      " 9.1200e+01 2.5451e+00 5.0000e+00 3.8400e+02 2.0900e+01 3.9123e+02\n",
      " 1.5550e+01]\n",
      "[5.3600e-02 2.1000e+01 5.6400e+00 0.0000e+00 4.3900e-01 6.5110e+00\n",
      " 2.1100e+01 6.8147e+00 4.0000e+00 2.4300e+02 1.6800e+01 3.9690e+02\n",
      " 5.2800e+00]\n",
      "[2.0608e-01 2.2000e+01 5.8600e+00 0.0000e+00 4.3100e-01 5.5930e+00\n",
      " 7.6500e+01 7.9549e+00 7.0000e+00 3.3000e+02 1.9100e+01 3.7249e+02\n",
      " 1.2500e+01]\n",
      "[3.1827e-01 0.0000e+00 9.9000e+00 0.0000e+00 5.4400e-01 5.9140e+00\n",
      " 8.3200e+01 3.9986e+00 4.0000e+00 3.0400e+02 1.8400e+01 3.9070e+02\n",
      " 1.8330e+01]\n",
      "[  1.20742   0.       19.58      0.        0.605     5.875    94.6\n",
      "   2.4259    5.      403.       14.7     292.29     14.43   ]\n",
      "[3.6150e-02 8.0000e+01 4.9500e+00 0.0000e+00 4.1100e-01 6.6300e+00\n",
      " 2.3400e+01 5.1167e+00 4.0000e+00 2.4500e+02 1.9200e+01 3.9690e+02\n",
      " 4.7000e+00]\n",
      "[  5.29305   0.       18.1       0.        0.7       6.051    82.5\n",
      "   2.1678   24.      666.       20.2     378.38     18.76   ]\n",
      "[  0.55007  20.        3.97      0.        0.647     7.206    91.6\n",
      "   1.9301    5.      264.       13.      387.89      8.1    ]\n",
      "[2.6938e-01 0.0000e+00 9.9000e+00 0.0000e+00 5.4400e-01 6.2660e+00\n",
      " 8.2800e+01 3.2628e+00 4.0000e+00 3.0400e+02 1.8400e+01 3.9339e+02\n",
      " 7.9000e+00]\n",
      "[3.1636e+00 0.0000e+00 1.8100e+01 0.0000e+00 6.5500e-01 5.7590e+00\n",
      " 4.8200e+01 3.0665e+00 2.4000e+01 6.6600e+02 2.0200e+01 3.3440e+02\n",
      " 1.4130e+01]\n",
      "[1.8337e-01 0.0000e+00 2.7740e+01 0.0000e+00 6.0900e-01 5.4140e+00\n",
      " 9.8300e+01 1.7554e+00 4.0000e+00 7.1100e+02 2.0100e+01 3.4405e+02\n",
      " 2.3970e+01]\n",
      "[9.2990e-02 0.0000e+00 2.5650e+01 0.0000e+00 5.8100e-01 5.9610e+00\n",
      " 9.2900e+01 2.0869e+00 2.0000e+00 1.8800e+02 1.9100e+01 3.7809e+02\n",
      " 1.7930e+01]\n",
      "[9.7440e-02 0.0000e+00 5.9600e+00 0.0000e+00 4.9900e-01 5.8410e+00\n",
      " 6.1400e+01 3.3779e+00 5.0000e+00 2.7900e+02 1.9200e+01 3.7756e+02\n",
      " 1.1410e+01]\n",
      "[  2.73397   0.       19.58      0.        0.871     5.597    94.9\n",
      "   1.5257    5.      403.       14.7     351.85     21.45   ]\n",
      "[8.79212e+00 0.00000e+00 1.81000e+01 0.00000e+00 5.84000e-01 5.56500e+00\n",
      " 7.06000e+01 2.06350e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.65000e+00\n",
      " 1.71600e+01]\n",
      "[  0.51183   0.        6.2       0.        0.507     7.358    71.6\n",
      "   4.148     8.      307.       17.4     390.07      4.73   ]\n",
      "[  2.36862   0.       19.58      0.        0.871     4.926    95.7\n",
      "   1.4608    5.      403.       14.7     391.71     29.53   ]\n",
      "[2.1161e-01 0.0000e+00 8.5600e+00 0.0000e+00 5.2000e-01 6.1370e+00\n",
      " 8.7400e+01 2.7147e+00 5.0000e+00 3.8400e+02 2.0900e+01 3.9447e+02\n",
      " 1.3440e+01]\n",
      "[7.40389e+00 0.00000e+00 1.81000e+01 0.00000e+00 5.97000e-01 5.61700e+00\n",
      " 9.79000e+01 1.45470e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.14640e+02\n",
      " 2.64000e+01]\n",
      "[1.9650e-02 8.0000e+01 1.7600e+00 0.0000e+00 3.8500e-01 6.2300e+00\n",
      " 3.1500e+01 9.0892e+00 1.0000e+00 2.4100e+02 1.8200e+01 3.4160e+02\n",
      " 1.2930e+01]\n",
      "[  3.67822   0.       18.1       0.        0.77      5.362    96.2\n",
      "   2.1036   24.      666.       20.2     380.79     10.19   ]\n",
      "[  9.82349   0.       18.1       0.        0.671     6.794    98.8\n",
      "   1.358    24.      666.       20.2     396.9      21.24   ]\n",
      "[5.4250e-02 0.0000e+00 4.0500e+00 0.0000e+00 5.1000e-01 6.3150e+00\n",
      " 7.3400e+01 3.3175e+00 5.0000e+00 2.9600e+02 1.6600e+01 3.9560e+02\n",
      " 6.2900e+00]\n",
      "[2.1161e-01 0.0000e+00 8.5600e+00 0.0000e+00 5.2000e-01 6.1370e+00\n",
      " 8.7400e+01 2.7147e+00 5.0000e+00 3.8400e+02 2.0900e+01 3.9447e+02\n",
      " 1.3440e+01]\n",
      "[  6.65492   0.       18.1       0.        0.713     6.317    83.\n",
      "   2.7344   24.      666.       20.2     396.9      13.99   ]\n",
      "[2.9850e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 6.4300e+00\n",
      " 5.8700e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9412e+02\n",
      " 5.2100e+00]\n",
      "[5.66998e+00 0.00000e+00 1.81000e+01 1.00000e+00 6.31000e-01 6.68300e+00\n",
      " 9.68000e+01 1.35670e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.75330e+02\n",
      " 3.73000e+00]\n",
      "[  1.05393   0.        8.14      0.        0.538     5.935    29.3\n",
      "   4.4986    4.      307.       21.      386.85      6.58   ]\n",
      "[  0.63796   0.        8.14      0.        0.538     6.096    84.5\n",
      "   4.4619    4.      307.       21.      380.02     10.26   ]\n",
      "[3.0410e-02 0.0000e+00 5.1900e+00 0.0000e+00 5.1500e-01 5.8950e+00\n",
      " 5.9600e+01 5.6150e+00 5.0000e+00 2.2400e+02 2.0200e+01 3.9481e+02\n",
      " 1.0560e+01]\n",
      "[1.4150e-01 0.0000e+00 6.9100e+00 0.0000e+00 4.4800e-01 6.1690e+00\n",
      " 6.6000e+00 5.7209e+00 3.0000e+00 2.3300e+02 1.7900e+01 3.8337e+02\n",
      " 5.8100e+00]\n",
      "[1.7446e-01 0.0000e+00 1.0590e+01 1.0000e+00 4.8900e-01 5.9600e+00\n",
      " 9.2100e+01 3.8771e+00 4.0000e+00 2.7700e+02 1.8600e+01 3.9325e+02\n",
      " 1.7270e+01]\n",
      "[3.77498e+00 0.00000e+00 1.81000e+01 0.00000e+00 6.55000e-01 5.95200e+00\n",
      " 8.47000e+01 2.87150e+00 2.40000e+01 6.66000e+02 2.02000e+01 2.20100e+01\n",
      " 1.71500e+01]\n",
      "[  2.14918   0.       19.58      0.        0.871     5.709    98.5\n",
      "   1.6232    5.      403.       14.7     261.95     15.79   ]\n",
      "[  2.44953   0.       19.58      0.        0.605     6.402    95.2\n",
      "   2.2625    5.      403.       14.7     330.04     11.32   ]\n",
      "[4.5900e-02 5.2500e+01 5.3200e+00 0.0000e+00 4.0500e-01 6.3150e+00\n",
      " 4.5600e+01 7.3172e+00 6.0000e+00 2.9300e+02 1.6600e+01 3.9690e+02\n",
      " 7.6000e+00]\n",
      "[2.9850e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 6.4300e+00\n",
      " 5.8700e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9412e+02\n",
      " 5.2100e+00]\n",
      "[4.011e-02 8.000e+01 1.520e+00 0.000e+00 4.040e-01 7.287e+00 3.410e+01\n",
      " 7.309e+00 2.000e+00 3.290e+02 1.260e+01 3.969e+02 4.080e+00]\n",
      "[7.5030e-02 3.3000e+01 2.1800e+00 0.0000e+00 4.7200e-01 7.4200e+00\n",
      " 7.1900e+01 3.0992e+00 7.0000e+00 2.2200e+02 1.8400e+01 3.9690e+02\n",
      " 6.4700e+00]\n",
      "[6.6420e-02 0.0000e+00 4.0500e+00 0.0000e+00 5.1000e-01 6.8600e+00\n",
      " 7.4400e+01 2.9153e+00 5.0000e+00 2.9600e+02 1.6600e+01 3.9127e+02\n",
      " 6.9200e+00]\n",
      "[1.1460e-01 2.0000e+01 6.9600e+00 0.0000e+00 4.6400e-01 6.5380e+00\n",
      " 5.8700e+01 3.9175e+00 3.0000e+00 2.2300e+02 1.8600e+01 3.9496e+02\n",
      " 7.7300e+00]\n",
      "[ 11.0874   0.      18.1      0.       0.718    6.411  100.       1.8589\n",
      "  24.     666.      20.2    318.75    15.02  ]\n",
      "[1.0328e-01 2.5000e+01 5.1300e+00 0.0000e+00 4.5300e-01 5.9270e+00\n",
      " 4.7200e+01 6.9320e+00 8.0000e+00 2.8400e+02 1.9700e+01 3.9690e+02\n",
      " 9.2200e+00]\n",
      "[1.06590e-01 8.00000e+01 1.91000e+00 0.00000e+00 4.13000e-01 5.93600e+00\n",
      " 1.95000e+01 1.05857e+01 4.00000e+00 3.34000e+02 2.20000e+01 3.76040e+02\n",
      " 5.57000e+00]\n",
      "[1.3110e-02 9.0000e+01 1.2200e+00 0.0000e+00 4.0300e-01 7.2490e+00\n",
      " 2.1900e+01 8.6966e+00 5.0000e+00 2.2600e+02 1.7900e+01 3.9593e+02\n",
      " 4.8100e+00]\n",
      "[3.768e-02 8.000e+01 1.520e+00 0.000e+00 4.040e-01 7.274e+00 3.830e+01\n",
      " 7.309e+00 2.000e+00 3.290e+02 1.260e+01 3.922e+02 6.620e+00]\n",
      "[3.3147e-01 0.0000e+00 6.2000e+00 0.0000e+00 5.0700e-01 8.2470e+00\n",
      " 7.0400e+01 3.6519e+00 8.0000e+00 3.0700e+02 1.7400e+01 3.7895e+02\n",
      " 3.9500e+00]\n",
      "[7.83932e+00 0.00000e+00 1.81000e+01 0.00000e+00 6.55000e-01 6.20900e+00\n",
      " 6.54000e+01 2.96340e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.96900e+02\n",
      " 1.32200e+01]\n",
      "[3.2264e-01 0.0000e+00 2.1890e+01 0.0000e+00 6.2400e-01 5.9420e+00\n",
      " 9.3500e+01 1.9669e+00 4.0000e+00 4.3700e+02 2.1200e+01 3.7825e+02\n",
      " 1.6900e+01]\n",
      "[ 22.5971   0.      18.1      0.       0.7      5.      89.5      1.5184\n",
      "  24.     666.      20.2    396.9     31.99  ]\n",
      "[5.4250e-02 0.0000e+00 4.0500e+00 0.0000e+00 5.1000e-01 6.3150e+00\n",
      " 7.3400e+01 3.3175e+00 5.0000e+00 2.9600e+02 1.6600e+01 3.9560e+02\n",
      " 6.2900e+00]\n",
      "[6.71772e+00 0.00000e+00 1.81000e+01 0.00000e+00 7.13000e-01 6.74900e+00\n",
      " 9.26000e+01 2.32360e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.20000e-01\n",
      " 1.74400e+01]\n",
      "[  0.40771   0.        6.2       1.        0.507     6.164    91.3\n",
      "   3.048     8.      307.       17.4     395.24     21.46   ]\n",
      "[4.5600e-02 0.0000e+00 1.3890e+01 1.0000e+00 5.5000e-01 5.8880e+00\n",
      " 5.6000e+01 3.1121e+00 5.0000e+00 2.7600e+02 1.6400e+01 3.9280e+02\n",
      " 1.3510e+01]\n",
      "[3.1130e-02 0.0000e+00 4.3900e+00 0.0000e+00 4.4200e-01 6.0140e+00\n",
      " 4.8500e+01 8.0136e+00 3.0000e+00 3.5200e+02 1.8800e+01 3.8564e+02\n",
      " 1.0530e+01]\n",
      "[1.1132e-01 0.0000e+00 2.7740e+01 0.0000e+00 6.0900e-01 5.9830e+00\n",
      " 8.3500e+01 2.1099e+00 4.0000e+00 7.1100e+02 2.0100e+01 3.9690e+02\n",
      " 1.3350e+01]\n",
      "[  2.924    0.      19.58     0.       0.605    6.101   93.       2.2834\n",
      "   5.     403.      14.7    240.16     9.81  ]\n",
      "[  1.38799   0.        8.14      0.        0.538     5.95     82.\n",
      "   3.99      4.      307.       21.      232.6      27.71   ]\n",
      "[  0.77299   0.        8.14      0.        0.538     6.495    94.4\n",
      "   4.4547    4.      307.       21.      387.94     12.8    ]\n",
      "[  8.15174   0.       18.1       0.        0.7       5.39     98.9\n",
      "   1.7281   24.      666.       20.2     396.9      20.85   ]\n",
      "[  0.40771   0.        6.2       1.        0.507     6.164    91.3\n",
      "   3.048     8.      307.       17.4     395.24     21.46   ]\n",
      "[1.9133e-01 2.2000e+01 5.8600e+00 0.0000e+00 4.3100e-01 5.6050e+00\n",
      " 7.0200e+01 7.9549e+00 7.0000e+00 3.3000e+02 1.9100e+01 3.8913e+02\n",
      " 1.8460e+01]\n",
      "[5.1880e-02 0.0000e+00 4.4900e+00 0.0000e+00 4.4900e-01 6.0150e+00\n",
      " 4.5100e+01 4.4272e+00 3.0000e+00 2.4700e+02 1.8500e+01 3.9599e+02\n",
      " 1.2860e+01]\n",
      "[5.70818e+00 0.00000e+00 1.81000e+01 0.00000e+00 5.32000e-01 6.75000e+00\n",
      " 7.49000e+01 3.33170e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.93070e+02\n",
      " 7.74000e+00]\n",
      "[  2.33099   0.       19.58      0.        0.871     5.186    93.8\n",
      "   1.5296    5.      403.       14.7     356.99     28.32   ]\n",
      "[  6.65492   0.       18.1       0.        0.713     6.317    83.\n",
      "   2.7344   24.      666.       20.2     396.9      13.99   ]\n",
      "[2.6169e-01 0.0000e+00 9.9000e+00 0.0000e+00 5.4400e-01 6.0230e+00\n",
      " 9.0400e+01 2.8340e+00 4.0000e+00 3.0400e+02 1.8400e+01 3.9630e+02\n",
      " 1.1720e+01]\n",
      "[2.7957e-01 0.0000e+00 9.6900e+00 0.0000e+00 5.8500e-01 5.9260e+00\n",
      " 4.2600e+01 2.3817e+00 6.0000e+00 3.9100e+02 1.9200e+01 3.9690e+02\n",
      " 1.3590e+01]\n",
      "[6.71772e+00 0.00000e+00 1.81000e+01 0.00000e+00 7.13000e-01 6.74900e+00\n",
      " 9.26000e+01 2.32360e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.20000e-01\n",
      " 1.74400e+01]\n",
      "[2.1770e-02 8.2500e+01 2.0300e+00 0.0000e+00 4.1500e-01 7.6100e+00\n",
      " 1.5700e+01 6.2700e+00 2.0000e+00 3.4800e+02 1.4700e+01 3.9538e+02\n",
      " 3.1100e+00]\n",
      "[3.0490e-02 5.5000e+01 3.7800e+00 0.0000e+00 4.8400e-01 6.8740e+00\n",
      " 2.8100e+01 6.4654e+00 5.0000e+00 3.7000e+02 1.7600e+01 3.8797e+02\n",
      " 4.6100e+00]\n",
      "[1.7446e-01 0.0000e+00 1.0590e+01 1.0000e+00 4.8900e-01 5.9600e+00\n",
      " 9.2100e+01 3.8771e+00 4.0000e+00 2.7700e+02 1.8600e+01 3.9325e+02\n",
      " 1.7270e+01]\n",
      "[6.6170e-02 0.0000e+00 3.2400e+00 0.0000e+00 4.6000e-01 5.8680e+00\n",
      " 2.5800e+01 5.2146e+00 4.0000e+00 4.3000e+02 1.6900e+01 3.8244e+02\n",
      " 9.9700e+00]\n",
      "[  8.98296   0.       18.1       1.        0.77      6.212    97.4\n",
      "   2.1222   24.      666.       20.2     377.73     17.6    ]\n",
      "[ 18.4982   0.      18.1      0.       0.668    4.138  100.       1.137\n",
      "  24.     666.      20.2    396.9     37.97  ]\n",
      "[1.0612e-01 3.0000e+01 4.9300e+00 0.0000e+00 4.2800e-01 6.0950e+00\n",
      " 6.5100e+01 6.3361e+00 6.0000e+00 3.0000e+02 1.6600e+01 3.9462e+02\n",
      " 1.2400e+01]\n",
      "[1.00623e+01 0.00000e+00 1.81000e+01 0.00000e+00 5.84000e-01 6.83300e+00\n",
      " 9.43000e+01 2.08820e+00 2.40000e+01 6.66000e+02 2.02000e+01 8.13300e+01\n",
      " 1.96900e+01]\n",
      "[3.2543e-01 0.0000e+00 2.1890e+01 0.0000e+00 6.2400e-01 6.4310e+00\n",
      " 9.8800e+01 1.8125e+00 4.0000e+00 4.3700e+02 2.1200e+01 3.9690e+02\n",
      " 1.5390e+01]\n",
      "[3.3045e-01 0.0000e+00 6.2000e+00 0.0000e+00 5.0700e-01 6.0860e+00\n",
      " 6.1500e+01 3.6519e+00 8.0000e+00 3.0700e+02 1.7400e+01 3.7675e+02\n",
      " 1.0880e+01]\n",
      "[6.0470e-02 0.0000e+00 2.4600e+00 0.0000e+00 4.8800e-01 6.1530e+00\n",
      " 6.8800e+01 3.2797e+00 3.0000e+00 1.9300e+02 1.7800e+01 3.8711e+02\n",
      " 1.3150e+01]\n",
      "[1.2329e-01 0.0000e+00 1.0010e+01 0.0000e+00 5.4700e-01 5.9130e+00\n",
      " 9.2900e+01 2.3534e+00 6.0000e+00 4.3200e+02 1.7800e+01 3.9495e+02\n",
      " 1.6210e+01]\n",
      "[8.49213e+00 0.00000e+00 1.81000e+01 0.00000e+00 5.84000e-01 6.34800e+00\n",
      " 8.61000e+01 2.05270e+00 2.40000e+01 6.66000e+02 2.02000e+01 8.34500e+01\n",
      " 1.76400e+01]\n",
      "[4.89822e+00 0.00000e+00 1.81000e+01 0.00000e+00 6.31000e-01 4.97000e+00\n",
      " 1.00000e+02 1.33250e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.75520e+02\n",
      " 3.26000e+00]\n",
      "[  0.63796   0.        8.14      0.        0.538     6.096    84.5\n",
      "   4.4619    4.      307.       21.      380.02     10.26   ]\n",
      "[2.8960e-01 0.0000e+00 9.6900e+00 0.0000e+00 5.8500e-01 5.3900e+00\n",
      " 7.2900e+01 2.7986e+00 6.0000e+00 3.9100e+02 1.9200e+01 3.9690e+02\n",
      " 2.1140e+01]\n",
      "[2.0090e-02 9.5000e+01 2.6800e+00 0.0000e+00 4.1610e-01 8.0340e+00\n",
      " 3.1900e+01 5.1180e+00 4.0000e+00 2.2400e+02 1.4700e+01 3.9055e+02\n",
      " 2.8800e+00]\n",
      "[  1.38799   0.        8.14      0.        0.538     5.95     82.\n",
      "   3.99      4.      307.       21.      232.6      27.71   ]\n",
      "[1.40507e+01 0.00000e+00 1.81000e+01 0.00000e+00 5.97000e-01 6.65700e+00\n",
      " 1.00000e+02 1.52750e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.50500e+01\n",
      " 2.12200e+01]\n",
      "[  2.44668   0.       19.58      0.        0.871     5.272    94.\n",
      "   1.7364    5.      403.       14.7      88.63     16.14   ]\n",
      "[  1.25179   0.        8.14      0.        0.538     5.57     98.1\n",
      "   3.7979    4.      307.       21.      376.57     21.02   ]\n",
      "[8.2650e-02 0.0000e+00 1.3920e+01 0.0000e+00 4.3700e-01 6.1270e+00\n",
      " 1.8400e+01 5.5027e+00 4.0000e+00 2.8900e+02 1.6000e+01 3.9690e+02\n",
      " 8.5800e+00]\n",
      "[8.7070e-02 0.0000e+00 1.2830e+01 0.0000e+00 4.3700e-01 6.1400e+00\n",
      " 4.5800e+01 4.0905e+00 5.0000e+00 3.9800e+02 1.8700e+01 3.8696e+02\n",
      " 1.0270e+01]\n",
      "[  6.80117   0.       18.1       0.        0.713     6.081    84.4\n",
      "   2.7175   24.      666.       20.2     396.9      14.7    ]\n",
      "[ 24.8017   0.      18.1      0.       0.693    5.349   96.       1.7028\n",
      "  24.     666.      20.2    396.9     19.77  ]\n",
      "[1.5010e-02 9.0000e+01 1.2100e+00 1.0000e+00 4.0100e-01 7.9230e+00\n",
      " 2.4800e+01 5.8850e+00 1.0000e+00 1.9800e+02 1.3600e+01 3.9552e+02\n",
      " 3.1600e+00]\n",
      "[1.5098e-01 0.0000e+00 1.0010e+01 0.0000e+00 5.4700e-01 6.0210e+00\n",
      " 8.2600e+01 2.7474e+00 6.0000e+00 4.3200e+02 1.7800e+01 3.9451e+02\n",
      " 1.0300e+01]\n",
      "[1.0000e-01 3.4000e+01 6.0900e+00 0.0000e+00 4.3300e-01 6.9820e+00\n",
      " 1.7700e+01 5.4917e+00 7.0000e+00 3.2900e+02 1.6100e+01 3.9043e+02\n",
      " 4.8600e+00]\n",
      "[  2.3004   0.      19.58     0.       0.605    6.319   96.1      2.1\n",
      "   5.     403.      14.7    297.09    11.1   ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Result: 7.971608836356955e+179"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = LinearRegressionSGD().fit(X_train, y_train)\n",
    "y_pred_sgd = sgd.predict(X_test)\n",
    "MAPE(y_test, y_pred_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lWappMdMtIPV"
   },
   "source": [
    "### Задание 3\n",
    "\n",
    "Вычислите веса по точной формуле, используя ```X_train``` и ```y_train```; предскажите с их помощью целевую переменную на ```X_test```, записав результат в переменную ```y_pred_lr``` и вычислите ошибку MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjMUlPje9-k0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result: 18.953134816375112"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_lr = X_test @ ((np.linalg.inv(X_train.T @ X_train) @ X_train.T) @ y_train)\n",
    "MAPE(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yL9L-4cwxZho"
   },
   "source": [
    "## Бонусное задание по неделе 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZFaUn7yx04u"
   },
   "source": [
    "До этого вы релизовывали модели, в которых не было штрафа за величину весов ```w```. Как было рассказано ранее в лекциях, это может привести к неустойчивости модели и переобучению. Чтобы избежать этих эффектов, предлагается добавить к оптимизируемому функционалу L2-норму весов; таким образом, будем решать задачу гребневой регрессии, Ridge:\n",
    "\n",
    "$$ \\frac{1}{l}(Xw-y)^T(Xw-y) +\\gamma||w||_2 \\rightarrow \\min_{w}. $$\n",
    "\n",
    "### Задание 11\n",
    "Реализуйте обучение такой модели в матричном виде (смотрите дополнительные материалы к этой неделе) с помощью стохастического градиентного спуска. Класс должен совпадать по набору реализованных функций с ```LinearRegressionVectorized```, разница будет лишь в параметре $\\gamma$, отвечающем за степень регуляризации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TEXqBqmGxWDz"
   },
   "outputs": [],
   "source": [
    "class RidgeSGD(BaseEstimator):\n",
    "    def __init__(self, epsilon=1e-4, max_steps=1000, w0=None, alpha=1e-2, gamma=0):\n",
    "        \"\"\"\n",
    "        epsilon: разница для нормы изменения весов \n",
    "        max_steps: максимальное количество шагов в градиентном спуске\n",
    "        w0: np.array (d,) - начальные веса\n",
    "        alpha: шаг обучения\n",
    "        gamma: коэффициент регуляризации\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.max_steps = max_steps\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.w = None\n",
    "        self.w_history = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        y: np.array (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        ---\n",
    "        output: np.array (l)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        y: np.array (l)\n",
    "        ---\n",
    "        output: np.array (d)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6t9rqXFu8Pq6"
   },
   "source": [
    "Так же, как и в основном задании, обучите модель с базовыми параметрами на тренировочных данных и сделайте прогноз y_pred_ridge. Выведите значение MAPE(y_test, y_pred_ridge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6A2hak_A8QPO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW04-sgd",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
